{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRETxIa1j/EmjIQwHv1WUI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya-0109/Art_Generation_with_Neural_Style_Transfer-DL/blob/main/Art_Gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yxPvuXAGgZG2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.ops import EagerTensor\n",
        "import pprint\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(272) # DO NOT CHANGE THIS VALUE\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "img_size = 400\n",
        "vgg = tf.keras.applications.VGG19(include_top=False,\n",
        "                                  input_shape=(img_size, img_size, 3),\n",
        "                                  weights='pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "\n",
        "vgg.trainable = False\n",
        "pp.pprint(vgg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "A-VfkFMKgjAD",
        "outputId": "c5911221-9feb-46fb-c687-e21fc0ca76d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c78346570a28>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrettyPrinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m vgg = tf.keras.applications.VGG19(include_top=False,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                   \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                   weights='pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/applications/vgg19.py\u001b[0m in \u001b[0;36mVGG19\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \"\"\"\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"imagenet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;34m\"The `weights` argument should be either \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;34m\"`None` (random initialization), `imagenet` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The `weights` argument should be either `None` (random initialization), `imagenet` (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: `weights=pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5.`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = Image.open(\"images/my_content.jpg\")\n",
        "#print(\"The content image (C) shows the Louvre museum's pyramid surrounded by old Paris buildings, against a sunny sky with a few clouds.\")\n",
        "content_image"
      ],
      "metadata": {
        "id": "wLHeMLSpgjc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_content_cost(content_output, generated_output):\n",
        "    \"\"\"\n",
        "    Computes the content cost\n",
        "\n",
        "    Arguments:\n",
        "    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C\n",
        "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n",
        "\n",
        "    Returns:\n",
        "    J_content -- scalar that you compute using equation 1 above.\n",
        "    \"\"\"\n",
        "    a_C = content_output[-1]\n",
        "    a_G = generated_output[-1]\n",
        "\n",
        "    ### START CODE HERE\n",
        "\n",
        "    # Retrieve dimensions from a_G (≈1 line)\n",
        "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
        "\n",
        "    # Reshape a_C and a_G (≈2 lines)\n",
        "    a_C_unrolled = tf.reshape(a_C, shape=[m, n_H * n_W, n_C]) # Or tf.reshape(a_C, shape=[m, -1 , n_C])\n",
        "    a_G_unrolled = tf.reshape(a_G, shape=[m, n_H * n_W, n_C]) # Or tf.reshape(a_G, shape=[m, -1 , n_C])\n",
        "\n",
        "    # compute the cost with tensorflow (≈1 line)\n",
        "    J_content =  tf.reduce_sum(tf.square(a_C_unrolled - a_G_unrolled))/(4.0 * n_H * n_W * n_C)\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return J_content"
      ],
      "metadata": {
        "id": "tSw1apHfiJwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "a_C = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
        "a_G = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
        "J_content = compute_content_cost([a_C], [a_G])\n",
        "J_content_0 = compute_content_cost([a_C], [a_C])\n",
        "assert type(J_content) == EagerTensor, \"Use the tensorflow function\"\n",
        "assert np.isclose(J_content_0, 0.0), \"Wrong value. compute_content_cost(A, A) must be 0\"\n",
        "assert np.isclose(J_content, 7.0568767), f\"Wrong value. Expected {7.0568767},  current{J_content}\"\n",
        "\n",
        "print(\"J_content = \" + str(J_content))\n",
        "\n",
        "print(\"\\033[92mAll tests passed\")"
      ],
      "metadata": {
        "id": "0ZigjrigiJ3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = Image.open(\"images/my_style.jpg\")\n",
        "example"
      ],
      "metadata": {
        "id": "1P7StXPDiJ6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2\n",
        "# GRADED FUNCTION: gram_matrix\n",
        "\n",
        "def gram_matrix(A):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    A -- matrix of shape (n_C, n_H*n_W)\n",
        "\n",
        "    Returns:\n",
        "    GA -- Gram matrix of A, of shape (n_C, n_C)\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    #(≈1 line)\n",
        "\n",
        "    GA = tf.matmul(A, tf.transpose(A))\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return GA\n"
      ],
      "metadata": {
        "id": "0fBkuCcEiR7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "A = tf.random.normal([3, 2 * 1], mean=1, stddev=4)\n",
        "GA = gram_matrix(A)\n",
        "\n",
        "assert type(GA) == EagerTensor, \"Use the tensorflow function\"\n",
        "assert GA.shape == (3, 3), \"Wrong shape. Check the order of the matmul parameters\"\n",
        "assert np.allclose(GA[0,:], [63.1888, -26.721275, -7.7320204]), \"Wrong values.\"\n",
        "\n",
        "print(\"GA = \\n\" + str(GA))\n",
        "\n",
        "print(\"\\033[92mAll tests passed\")"
      ],
      "metadata": {
        "id": "UmuzN2opiR-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C3\n",
        "# GRADED FUNCTION: compute_layer_style_cost\n",
        "\n",
        "def compute_layer_style_cost(a_S, a_G):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S\n",
        "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n",
        "\n",
        "    Returns:\n",
        "    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "\n",
        "    # Retrieve dimensions from a_G (≈1 line)\n",
        "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
        "\n",
        "    # Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)\n",
        "    a_S = tf.transpose(tf.reshape(a_S, shape=[-1, n_C]))\n",
        "    # OR a_S = tf.transpose(tf.reshape(a_S, shape=[ n_H * n_W, n_C]))\n",
        "    a_G = tf.transpose(tf.reshape(a_G, shape=[-1, n_C]))\n",
        "    # Computing gram_matrices for both images S and G (≈2 lines)\n",
        "    GS = gram_matrix(a_S)\n",
        "    GG = gram_matrix(a_G)\n",
        "\n",
        "    # Computing the loss (≈1 line)\n",
        "    J_style_layer = tf.reduce_sum(tf.square(GS - GG))/(4.0 *(( n_H * n_W * n_C)**2))\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return J_style_layer"
      ],
      "metadata": {
        "id": "ZyGF6kiGiSBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "a_S = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
        "a_G = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
        "J_style_layer_GG = compute_layer_style_cost(a_G, a_G)\n",
        "J_style_layer_SG = compute_layer_style_cost(a_S, a_G)\n",
        "\n",
        "\n",
        "assert type(J_style_layer_GG) == EagerTensor, \"Use the tensorflow functions\"\n",
        "assert np.isclose(J_style_layer_GG, 0.0), \"Wrong value. compute_layer_style_cost(A, A) must be 0\"\n",
        "assert J_style_layer_SG > 0, \"Wrong value. compute_layer_style_cost(A, B) must be greater than 0 if A != B\"\n",
        "assert np.isclose(J_style_layer_SG, 14.017805), \"Wrong value.\"\n",
        "\n",
        "print(\"J_style_layer = \" + str(J_style_layer_SG))\n",
        "\n",
        "print(\"\\033[92mAll tests passed\")"
      ],
      "metadata": {
        "id": "rNjJSNPcig9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in vgg.layers:\n",
        "    print(layer.name)"
      ],
      "metadata": {
        "id": "06qLPt7jihC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg.get_layer('block5_conv4').output"
      ],
      "metadata": {
        "id": "s3vBAQ9pihJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STYLE_LAYERS = [\n",
        "    ('block1_conv1', 1.0),\n",
        "    ('block2_conv1', 0.8),\n",
        "    ('block3_conv1', 0.7),\n",
        "    ('block4_conv1', 0.2),\n",
        "    ('block5_conv1', 0.1)]"
      ],
      "metadata": {
        "id": "gpz1D0Qtiogu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_style_cost(style_image_output, generated_image_output, STYLE_LAYERS=STYLE_LAYERS):\n",
        "    \"\"\"\n",
        "    Computes the overall style cost from several chosen layers\n",
        "\n",
        "    Arguments:\n",
        "    style_image_output -- our tensorflow model\n",
        "    generated_image_output --\n",
        "    STYLE_LAYERS -- A python list containing:\n",
        "                        - the names of the layers we would like to extract style from\n",
        "                        - a coefficient for each of them\n",
        "\n",
        "    Returns:\n",
        "    J_style -- tensor representing a scalar value, style cost defined above by equation (2)\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize the overall style cost\n",
        "    J_style = 0\n",
        "\n",
        "    # Set a_S to be the hidden layer activation from the layer we have selected.\n",
        "    # The first element of the array contains the input layer image, which must not to be used.\n",
        "    a_S = style_image_output[1:]\n",
        "\n",
        "    # Set a_G to be the output of the choosen hidden layers.\n",
        "    # The First element of the list contains the input layer image which must not to be used.\n",
        "    a_G = generated_image_output[1:]\n",
        "    for i, weight in zip(range(len(a_S)), STYLE_LAYERS):\n",
        "        # Compute style_cost for the current layer\n",
        "        J_style_layer = compute_layer_style_cost(a_S[i], a_G[i])\n",
        "\n",
        "        # Add weight * J_style_layer of this layer to overall style cost\n",
        "        J_style += weight[1] * J_style_layer\n",
        "\n",
        "    return J_style\n"
      ],
      "metadata": {
        "id": "4e3KWgkuiokP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C4\n",
        "# GRADED FUNCTION: total_cost\n",
        "@tf.function()\n",
        "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
        "    \"\"\"\n",
        "    Computes the total cost function\n",
        "\n",
        "    Arguments:\n",
        "    J_content -- content cost coded above\n",
        "    J_style -- style cost coded above\n",
        "    alpha -- hyperparameter weighting the importance of the content cost\n",
        "    beta -- hyperparameter weighting the importance of the style cost\n",
        "\n",
        "    Returns:\n",
        "    J -- total cost as defined by the formula above.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "\n",
        "    #(≈1 line)\n",
        "    J = alpha * J_content + beta * J_style\n",
        "\n",
        "    ### START CODE HERE\n",
        "\n",
        "    return J"
      ],
      "metadata": {
        "id": "lR6pjJHhiop8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "J_content = 0.2\n",
        "J_style = 0.8\n",
        "J = total_cost(J_content, J_style)\n",
        "\n",
        "assert type(J) == EagerTensor, \"Do not remove the @tf.function() modifier from the function\"\n",
        "assert J == 34, \"Wrong value. Try inverting the order of alpha and beta in the J calculation\"\n",
        "assert np.isclose(total_cost(0.3, 0.5, 3, 8), 4.9), \"Wrong value. Use the alpha and beta parameters\"\n",
        "\n",
        "np.random.seed(1)\n",
        "print(\"J = \" + str(total_cost(np.random.uniform(0, 1), np.random.uniform(0, 1))))\n",
        "\n",
        "print(\"\\033[92mAll tests passed\")"
      ],
      "metadata": {
        "id": "SGrNkW9eios4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = np.array(Image.open(\"images/my_content.jpg\").resize((img_size, img_size)))\n",
        "content_image = tf.constant(np.reshape(content_image, ((1,) + content_image.shape)))\n",
        "\n",
        "print(content_image.shape)\n",
        "imshow(content_image[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-V-1muh3i069"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "style_image =  np.array(Image.open(\"images/my_style.jpg\").resize((img_size, img_size)))\n",
        "style_image = tf.constant(np.reshape(style_image, ((1,) + style_image.shape)))\n",
        "\n",
        "print(style_image.shape)\n",
        "imshow(style_image[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gLoBju0Ai0-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
        "noise = tf.random.uniform(tf.shape(generated_image), 0, 0.8)\n",
        "generated_image = tf.add(generated_image, noise)\n",
        "generated_image = tf.clip_by_value(generated_image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "print(generated_image.shape)\n",
        "imshow(generated_image.numpy()[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oSeP5OyGjADX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_layer_outputs(vgg, layer_names):\n",
        "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
        "    # layer_names has 'layer' elements in it.\n",
        "    outputs = [vgg.get_layer(layer[0]).output for layer in layer_names]\n",
        "\n",
        "    model = tf.keras.Model([vgg.input], outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "sbmRgzNajAIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "content_layer = [('block5_conv4', 1)]\n",
        "\n",
        "vgg_model_outputs = get_layer_outputs(vgg, STYLE_LAYERS + content_layer)"
      ],
      "metadata": {
        "id": "5j8vRpP3jGGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "content_target = vgg_model_outputs(content_image)  # Content encoder\n",
        "style_targets = vgg_model_outputs(style_image)     # Style enconder"
      ],
      "metadata": {
        "id": "L_Pe1hfxjGKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the content image to be the input of the VGG model.\n",
        "# Set a_C to be the hidden layer activation from the layer we have selected\n",
        "preprocessed_content =  tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
        "a_C = vgg_model_outputs(preprocessed_content)\n",
        "\n",
        "# Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2']\n",
        "# and isn't evaluated yet. Later in the code, we'll assign the image G as the model input.\n",
        "a_G = vgg_model_outputs(generated_image)\n",
        "\n",
        "# Compute the content cost\n",
        "J_content = compute_content_cost(a_C, a_G)\n",
        "\n",
        "print(J_content)"
      ],
      "metadata": {
        "id": "1c5PPrnNjGNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the input of the model to be the \"style\" image\n",
        "preprocessed_style =  tf.Variable(tf.image.convert_image_dtype(style_image, tf.float32))\n",
        "a_S = vgg_model_outputs(preprocessed_style)\n",
        "\n",
        "# Compute the style cost\n",
        "J_style = compute_style_cost(a_S, a_G)\n",
        "print(J_style)"
      ],
      "metadata": {
        "id": "w_8FN9lHjGQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_0_1(image):\n",
        "    \"\"\"\n",
        "    Truncate all the pixels in the tensor to be between 0 and 1\n",
        "\n",
        "    Arguments:\n",
        "    image -- Tensor\n",
        "    J_style -- style cost coded above\n",
        "\n",
        "    Returns:\n",
        "    Tensor\n",
        "    \"\"\"\n",
        "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "def tensor_to_image(tensor):\n",
        "    \"\"\"\n",
        "    Converts the given tensor into a PIL image\n",
        "\n",
        "    Arguments:\n",
        "    tensor -- Tensor\n",
        "\n",
        "    Returns:\n",
        "    Image: A PIL image\n",
        "    \"\"\"\n",
        "    tensor = tensor * 255\n",
        "    tensor = np.array(tensor, dtype=np.uint8)\n",
        "    if np.ndim(tensor) > 3:\n",
        "        assert tensor.shape[0] == 1\n",
        "        tensor = tensor[0]\n",
        "    return Image.fromarray(tensor)"
      ],
      "metadata": {
        "id": "1HWmiBlLjScG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C5\n",
        "# GRADED FUNCTION: train_step\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.03)\n",
        "\n",
        "@tf.function()\n",
        "def train_step(generated_image, alpha = 10, beta = 40):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # In this function you must use the precomputed encoded images a_S and a_C\n",
        "        # Compute a_G as the vgg_model_outputs for the current generated image\n",
        "\n",
        "        ### START CODE HERE\n",
        "\n",
        "        #(1 line)\n",
        "        a_G = vgg_model_outputs(generated_image)\n",
        "\n",
        "        # Compute the style cost\n",
        "        #(1 line)\n",
        "        J_style = compute_style_cost(a_S, a_G)\n",
        "\n",
        "        #(2 lines)\n",
        "        # Compute the content cost\n",
        "        J_content = compute_content_cost(a_C, a_G)\n",
        "        # Compute the total cost\n",
        "        J = total_cost(J_content, J_style,alpha = alpha, beta = beta)\n",
        "\n",
        "        ### END CODE HERE\n",
        "\n",
        "    grad = tape.gradient(J, generated_image)\n",
        "\n",
        "    optimizer.apply_gradients([(grad, generated_image)])\n",
        "    generated_image.assign(clip_0_1(generated_image))\n",
        "    # For grading purposes\n",
        "    return J"
      ],
      "metadata": {
        "id": "GvnrpvpZjSiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You always must run the last cell before this one. You will get an error if not.\n",
        "generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
        "\n",
        "J1 = train_step(generated_image)\n",
        "print(J1)\n",
        "assert type(J1) == EagerTensor, f\"Wrong type {type(J1)} != {EagerTensor}\"\n",
        "#assert np.isclose(J1, 10221.168), f\"Unexpected cost for epoch 0: {J1} != {10221.168}\"\n",
        "\n",
        "J2 = train_step(generated_image)\n",
        "print(J2)\n",
        "#assert np.isclose(J2, 6081.23541, rtol=0.05), f\"Unexpected cost for epoch 1: {J2} != {6081.2354}\"\n",
        "\n",
        "print(\"\\033[92mAll tests passed\")"
      ],
      "metadata": {
        "id": "5TKhUmj4jSlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the generated image at some epochs\n",
        "# Uncoment to reset the style transfer process. You will need to compile the train_step function again\n",
        "\n",
        "generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
        "\n",
        "epochs = 2501\n",
        "for i in range(epochs):\n",
        "    train_step(generated_image,alpha = 100, beta = 10**2)\n",
        "    if i % 250 == 0:\n",
        "        print(f\"Epoch {i} \")\n",
        "    if i % 250 == 0:\n",
        "        image = tensor_to_image(generated_image)\n",
        "        imshow(image)\n",
        "        image.save(f\"output/image_{i}.jpg\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "evrvEFSUjbJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the 3 images in a row\n",
        "fig = plt.figure(figsize=(16, 4))\n",
        "ax = fig.add_subplot(1, 3, 1)\n",
        "imshow(content_image[0])\n",
        "ax.title.set_text('Content image')\n",
        "ax = fig.add_subplot(1, 3, 2)\n",
        "imshow(style_image[0])\n",
        "ax.title.set_text('Style image')\n",
        "ax = fig.add_subplot(1, 3, 3)\n",
        "imshow(generated_image[0])\n",
        "ax.title.set_text('Generated image')\n",
        "plt.show()\n",
        "plt.savefig(\"My_Result.jpg\")"
      ],
      "metadata": {
        "id": "7dv4TWmnjbNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(generated_image[0])\n",
        "plt.savefig('images/my_enerated_image.png')\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "7KHCj-cEjjPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the 3 images in a row\n",
        "fig = plt.figure(figsize=(16, 4))\n",
        "ax = fig.add_subplot(1, 3, 1)\n",
        "imshow(content_image[0])\n",
        "ax.title.set_text('Content image')\n",
        "ax = fig.add_subplot(1, 3, 2)\n",
        "imshow(style_image[0])\n",
        "ax.title.set_text('Style image')\n",
        "ax = fig.add_subplot(1, 3, 3)\n",
        "imshow(generated_image[0])\n",
        "ax.title.set_text('Generated image')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3EOL-mlQjjSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qbQrEmqjjWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}